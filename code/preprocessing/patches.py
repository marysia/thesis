import os

import numpy as np

from basedata import BaseData, Data


class DataPatches(BaseData):
    def __init__(self, args):
        self.name = 'fp-reduction-patches'
        self.shape = args.shape
        self.zoomed = args.zoomed
        self.samples = args.samples

        # quick hack for 2d patches data
        if '2d' in args.data:
            shape = list(args.shape)
            shape[0] = 1
            self.shape = tuple(shape)

        self.train_dataset = args.train
        self.val_dataset = args.val
        self.test_dataset = args.test

        self.train_mean = None
        self.train_std = None

        BaseData.__init__(self)

    def load(self):
        """
        Sets class variables train, val and test to contain a Data class instance with the data.
        """
        self.train = self.get_dataset(self.train_dataset, 'train')
        self.val = self.get_dataset(self.val_dataset, 'val')
        self.test = self.get_dataset(self.test_dataset, 'test')

    def get_dataset(self, dataset, scope):
        """
        Returns a Data class instance with specified dataset and scope.
        """
        if dataset == 'empty':
            return Data(scope='%s-empty' % scope, x=np.array([]), y=np.array([]), id=np.array([]), balanced=None)

        if dataset == 'nlst-balanced':
            return self.nlst(scope, True)

        if dataset == 'nlst-unbalanced':
            return self.nlst(scope, False)

        if dataset == 'lidc-localization':
            return self.lidc_localization(scope, False)

    def nlst(self, scope, balanced):
        """
        NLST dataset. Candidate patches as generated by Aidence for training for false positive reduction. Balanced
        form generally functions as training set, while unbalanced form as validation set.
        """
        f_scope = 'train' if scope == 'train' else 'test'

        datadir = '/home/marysia/data/thesis/patches/nlst-patches/'

        prefix = 'all_' if not balanced else ''
        datadir = datadir.replace('thesis/patches', 'thesis/zoomed-patches') if self.zoomed else datadir

        pos = np.load(os.path.join(datadir, 'positive_%s_patches.npz' % (f_scope)))['data']
        neg = np.load(os.path.join(datadir, '%snegative_%s_patches.npz' % (prefix, f_scope)))['data']
        data = np.concatenate([pos, neg])
        data = self.preprocess(data, scope)
        labels = np.concatenate([np.ones(pos.shape[0]), np.zeros(neg.shape[0])])

        data, labels, p = self.shuffle(data, labels)

        if scope == 'train' and self.samples != -100:
            data = data[:self.samples]
            labels = labels[:self.samples]
            p = p[:self.samples]

        return Data(scope=scope, x=data, y=self._one_hot_encoding(labels, self.nb_classes), id=p, balanced=balanced)

    def lidc_localization(self, scope, balanced):
        """
        LIDC localization data. Candidate patches generated by the Aidence algorithm. This data generally functions
        as the test set.
        """
        datadir = '/home/marysia/data/thesis/patches/lidc-localization-patches/'
        datadir = datadir.replace('thesis/patches', 'thesis/zoomed-patches') if self.zoomed else datadir

        pos = np.load(os.path.join(datadir, 'positive_patches.npz'))['data']
        neg = np.load(os.path.join(datadir, 'negative_patches.npz'))['data']

        data = np.concatenate([pos, neg])
        data = self.preprocess(data, scope)
        labels = np.concatenate([np.ones(pos.shape[0]), np.zeros(neg.shape[0])])

        data, labels, p = self.shuffle(data, labels)

        if scope == 'train' and self.samples != -100:
            data = data[:self.samples]
            labels = labels[:self.samples]
            p = p[:self.samples]

        return Data(scope=scope, x=data, y=self._one_hot_encoding(labels, self.nb_classes), id=p, balanced=balanced)

    def _data_reshape(self, data):
        """
        Resize the data to a smaller patch, e.g. 13x120x120 to 8x30x30, by determining the center point and cutting
        around it.
        """
        data_offset = [int(size / 2) for size in data.shape[1:]]
        data_diff = [int(size / 2) for size in self.shape]
        data_diff_min = data_diff
        data_diff_max = []
        for i, elem in enumerate(data_diff):
            if self.shape[i] % 2 == 0:
                data_diff_max.append(elem)
            else:
                data_diff_max.append(elem + 1)
        data = data[:, (data_offset[0] - data_diff_min[0]):(data_offset[0] + data_diff_max[0]),
               (data_offset[1] - data_diff_min[1]):(data_offset[1] + data_diff_max[1]),
               (data_offset[2] - data_diff_min[2]):(data_offset[2] + data_diff_max[2])]

        if data.shape[1] == 1:
            data = data.reshape(data.shape[0], data.shape[2], data.shape[3])
        return data

    def preprocess(self, data, scope):
        """
        Preprocess the data by reshaping it to the target shape, normalizing it between values of [-1, 1],
        subtracting the train mean and dividing by the std, and reshaping it to contain the channel.
        """
        # reshape
        data = self._data_reshape(data)

        # normalize
        if data.dtype == np.int16:
            start_unit = -1000
            end_unit = 300
            data = 2 * (data.astype(np.float32) - start_unit) / (end_unit - start_unit) - 1

        # subtract train mean and divide by train std
        if scope == 'train':
            self.mean = np.mean(data)
            data -= self.mean
            self.std = np.std(data)
            data /= self.std
        else:
            data -= self.mean
            data /= self.std

        # reshape for channel
        s = data.shape
        if len(data.shape) == 4:
            data = data.reshape((s[0], s[1], s[2], s[3], 1))
        else:
            data = data.reshape((s[0], s[1], s[2], 1))
        return data

    def _set_classes(self):
        self.nb_classes = 2  # positive or negative
