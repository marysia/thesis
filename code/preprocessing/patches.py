import os
import sys

sys.path.append('..')

import numpy as np

from basedata import BaseData, Data
from utils.config import DATADIR

class DataPatches(BaseData):
    def __init__(self, args):
        self.name = 'fp-reduction-patches'
        samples = [int(sample)for sample in args.samples]
        self.small = True if max(samples) <= 5000 and 'unbalanced' not in args.train else False
        self.shape = [int(dim) for dim in args.shape]
        self.zoomed = args.zoomed

        self.train_dataset = args.train
        self.val_dataset = args.val
        self.test_dataset = args.test

        self.train_mean = None
        self.train_std = None

        BaseData.__init__(self)

    def load(self):
        """
        Sets class variables train, val and test to contain a Data class instance with the data.
        """
        self.train = self.get_dataset(self.train_dataset, 'train')
        print('Train patches loaded.')
        self.val = self.get_dataset(self.val_dataset, 'val')
        print('Validation patches loaded.')
        self.test = self.get_dataset(self.test_dataset, 'test')
        print('Test patches loaded.')

    def get_dataset(self, dataset, scope):
        """
        Returns a Data class instance with specified dataset and scope.
        """
        if dataset == 'empty':
            return Data(scope='%s-empty' % scope, x=np.array([]), y=np.array([]), nb_classes=2, balanced=None)

        if dataset == 'nlst-balanced':
            return self.nlst(scope, True)

        if dataset == 'nlst-unbalanced':
            return self.nlst(scope, False)

        if dataset == 'lidc-localization':
            return self.lidc_localization(scope, False)

    def nlst(self, scope, balanced):
        """
        NLST dataset. Candidate patches as generated by Aidence for training for false positive reduction. Balanced
        form generally functions as training set, while unbalanced form as validation set.
        """
        # determine correct names
        datadir = os.path.join(DATADIR, 'patches', 'nlst-patches')
        f_scope = 'train' if scope == 'train' else 'test'

        small = 'small_' if f_scope == 'train' and self.small else ''
        all = 'all_' if not balanced else ''
        datadir = datadir.replace('thesis/patches', 'thesis/zoomed-patches') if self.zoomed else datadir

        # Load, preprocess, shuffle and return
        if self.zoomed:
            pos = np.load(os.path.join(datadir, 'positive_%s_patches.npz' % (f_scope)))['data']
            neg = np.load(os.path.join(datadir, 'negative_%s_patches.npz' % (f_scope)))['data']
        else:
            pos = np.load(os.path.join(datadir, '%spositive_%s_patches.npz' % (small, f_scope)))['data']
            neg = np.load(os.path.join(datadir, '%s%snegative_%s_patches.npz' % (all, small, f_scope)))['data']
        data = np.concatenate([pos, neg])
        data = self.preprocess(data, scope)
        labels = np.concatenate([np.ones(pos.shape[0]), np.zeros(neg.shape[0])])

        return Data(scope=scope, x=data, y=labels, nb_classes = self.nb_classes, balanced=True)

    def lidc_localization(self, scope, balanced):
        """
        LIDC localization data. Candidate patches generated by the Aidence algorithm. This data generally functions
        as the test set.
        """
        datadir = os.path.join(DATADIR, 'patches', 'lidc-localization-patches')
        datadir = datadir.replace('thesis/patches', 'thesis/zoomed-patches') if self.zoomed else datadir

        pos = np.load(os.path.join(datadir, 'positive_patches.npz'))['data']
        neg = np.load(os.path.join(datadir, 'negative_patches.npz'))['data']

        data = np.concatenate([pos, neg])
        data = self.preprocess(data, scope)
        labels = np.concatenate([np.ones(pos.shape[0]), np.zeros(neg.shape[0])])

        return Data(scope=scope, x=data, y=labels, nb_classes=self.nb_classes, balanced=balanced)

    def _data_reshape(self, data):
        """
        Resize the data to a smaller patch, e.g. 13x120x120 to 8x30x30, by determining the center point and cutting
        around it.
        """
        data_offset = [int(size / 2) for size in data.shape[1:]]
        data_diff = [int(size / 2) for size in self.shape]
        data_diff_min = data_diff
        data_diff_max = []
        for i, elem in enumerate(data_diff):
            if self.shape[i] % 2 == 0:
                data_diff_max.append(elem)
            else:
                data_diff_max.append(elem + 1)
        data = data[:, (data_offset[0] - data_diff_min[0]):(data_offset[0] + data_diff_max[0]),
               (data_offset[1] - data_diff_min[1]):(data_offset[1] + data_diff_max[1]),
               (data_offset[2] - data_diff_min[2]):(data_offset[2] + data_diff_max[2])]

        if data.shape[1] == 1:
            data = data.reshape(data.shape[0], data.shape[2], data.shape[3])
        return data

    def preprocess(self, data, scope):
        """
        Preprocess the data by reshaping it to the target shape, normalizing it between values of [-1, 1],
        subtracting the train mean and dividing by the std, and reshaping it to contain the channel.
        """
        if scope != 'train':
            # reshape
            data = self._data_reshape(data)

        # normalize
        if data.dtype == np.int16:
            start_unit = -1000
            end_unit = 300
            data = 2 * (data.astype(np.float32) - start_unit) / (end_unit - start_unit) - 1

        # subtract train mean and divide by train std
        if scope == 'train':
            self.mean = np.mean(data)
            data -= self.mean
            self.std = np.std(data)
            data /= self.std
        else:
            data -= self.mean
            data /= self.std

        # reshape for channel
        s = data.shape
        if len(data.shape) == 4:
            data = data.reshape((s[0], s[1], s[2], s[3], 1))
        else:
            data = data.reshape((s[0], s[1], s[2], 1))
        return data

    def _set_classes(self):
        self.nb_classes = 2  # positive or negative
