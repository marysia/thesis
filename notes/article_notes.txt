Article Notes


-- Exploiting Cyclic Symmetry in Convolutional Neural Networks  
--- Dieleman, De Fauw, Kavukcuoglu

	The authors have introduced four operations that can be inserted into neural network models as layers for building rotation equivariant neural networks. Beyond adapting the minibatch size used for training, no modifications are required. 

The four operations are: 
	- Slice:	stack rotated copies of set of input examples into a single minibatch (minibatch x4 larger)
	- Pool: 	combines predictions from different rotated copies using permutation-invariant pooling (minibatch 4x smaller)
	- Stack:	concatenate feature maps obtained from different orientations
	- Roll:		apply T (stacking operation) to all possible cyclic permutations of the input and stacking the results along the batch dimension

To achieve 4-way parameter sharing, there are two implementations: roate the feature maps or rotate the filters. The authors have chosen for rotating the feature maps, as it is the easiest to implement. The feature maps must necessarily be square; the filters need not be. 

For future work, the authors suggest to apply their approach to other types of data which exhibit rotational symmetry, particularly in domains where data scarcity is often an issue (e.g. medical imaging.)


-- Rotation, Scaling and Deformation Invariant Scattering for Texture Disctimination
--- Sifre, mallat

	The authors show that affine invariant representations can be obtained with a scattering operator defined on the translation, rotation and scaling groups, while keeping enough discriminative information.  A scattering operatore computes an invariant image representation relatively to the action of a group. The filters are not learned, but are scaled and rotated wavelets. 

The invariance properties of this scattering image patch representation can also replace SIFT type features for more complex classification problems. 


-- Deep Symmetry Networks
--- Gens, Domingos

As convnets can achieve a degree of translational invariance, but cannot handle other groups, these groups effects have to be approximated by small translations. The authors propose deep symmetry networks (symnets), a generalization that forms feature maps over arbitrary symmetry groups. The latter approach greatly reduce sample complexity relatively to the former approach. 

The authors approximate the high-dimensional feature map required to extend convnets to affine spaces by using kernel functions that interpolate and control pooling in the feature maps. Sample complexity is reduced in both 2D and 3D transformations.

They model multiple symmetries jointly in each layer and do not completely pool out a symmetry, which has the advantage that subparts of objects may have relative flexibility, but not total invariance along certain dimensions of symmetry space. 

Lie groups: continuous symmetry groups whose elements form a smooth differentiable manifold
Affine group: set of transformations that preserves collinearity and parallel lines. 
Euclidean group: subgroup of affine that preserves distances and includes the set of rigid body motions (translations and rotations) in three-dimensional space.

-- Spatial Transformer Networks
--- Jaderberg, Simonyan, Zisserman, Kavukcuoglu
The authors propose the Spatial Transformer, a learnable model which can be inserted into existing convolutional architectures and explicitly allows spatial manipulatioon of data within the network. Without training or modifications to the optimisation process, the neural network can spatially transform feature maps. 

Whilst max-pooling layers allows a network to be somewhat spatially invariant, this is only realised over a deep hierarchy of max-pooling and convolutions. The intermediate feature maps are not invariant to large transformations of the input data.  

The spatial transformer module provides spatial transformation capabilities which it learns during training for the task in question. The regressed transformation parameters from the spatial transformer are available as an output and could be used for subsequent tasks.

-- Transformation Equivariant Boltzmann Machines
--- Kivinen, Williams

Translational equivariance is available in neural networks and RBMs, but the authors seek to also include rotational equivariance in their Deep Belief Network. Unlike the translational and rotational invariant architecture proposed by Fidler and Leonardis, the authors' architecture is not heavily dependent on various thresholds used in the learning algorithm, enabling it to carry out bottom-up/top-down inference in the face of ambiguous input or missing data. 

They describe the rotation-equivariant restricted Boltzmann machine, the translation equivariant version of the model, and how this is generalized to a deep belief net.  

